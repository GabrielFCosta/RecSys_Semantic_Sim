{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuVfPtGXk92ILzplRVNoqb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielFCosta/TCC-BSI-2023.1/blob/main/old/matc97_gabriel_teste_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g1zLg3Ipx77",
        "outputId": "5da04a09-a240-4ffa-abba-9dce32c9c684"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import time as tm\n",
        "from urllib.request import urlopen\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWcc_InWq1HI",
        "outputId": "93815740-0cfc-46d3-b2e2-d326827a48e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove pontuação, utilizada pela função seguinte de pré-processamento\n",
        "def remover_pontuacao(tokens):\n",
        "  for token in tokens:\n",
        "    if token in string.punctuation:\n",
        "      tokens.remove(token)\n",
        "# Função de pré-processamento\n",
        "def preprocess(text):\n",
        "    text = word_tokenize(text)\n",
        "    remover_pontuacao(text)\n",
        "    return pos_tag(text)\n",
        "\n",
        "# Listas para coletar rótulos e entidades (palavras) do texto\n",
        "codes = []\n",
        "words = []\n",
        "def  clearlists():\n",
        "  if len(words) > 0:\n",
        "    words.clear()\n",
        "  if len(codes) > 0:\n",
        "    codes.clear()\n",
        "\n",
        "# Função de reconhecimento de entidades nomeadas (NER)\n",
        "def ner(sent):\n",
        "  ne_tree = nltk.ne_chunk(sent)\n",
        "  for chunk in ne_tree:\n",
        "    if hasattr(chunk,\"label\"):\n",
        "      word =' '.join(c[0] for c in chunk)\n",
        "      #print(chunk.label(),word)\n",
        "      codes.append(chunk.label())\n",
        "      words.append(word)\n",
        "\n",
        "# Função que retorna dataframe de entidades (palavras), rótulos e frequências \n",
        "def returndataset(col1,col2):\n",
        "  # junta listas de palavras e rótulos num dataframe\n",
        "  dataset = pd.DataFrame(zip(words,codes),columns =[col1,col2])\n",
        "  clearlists()\n",
        "  # calcula frequências das palavras e adiciona nova coluna\n",
        "  dataset['freq'] = dataset.groupby([col1,col2])[col1].transform('count')\n",
        "  return dataclean(dataset)\n",
        " \n",
        "# Remove linhas duplicatas e atualiza index\n",
        "def dataclean(dataset):\n",
        "  dataset.drop_duplicates(inplace= True)\n",
        "  dataset.reset_index()\n",
        "  return dataset\n",
        "\n",
        "# Verifica rótulo gramatical\n",
        "def test(tag):\n",
        "  # verificando todos substantivos, adjetivos, verbos e numerais cardinais.\n",
        "  #tags = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ','CD']\n",
        "  tags = ['NN','NNS','NNP','NNPS']\n",
        "  for i in tags:\n",
        "    if i == tag:\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "# Filtra palavras da lista gerada pelo pos tagging e retorna dataframe ou lista de palavras\n",
        "def filtertuples(textlist,rtdata):\n",
        "  for tuple in textlist:\n",
        "    if test(tuple[1]):\n",
        "      words.append(tuple[0])\n",
        "      codes.append(tuple[1])\n",
        "  if rtdata:\n",
        "    return returndataset('word','pos_tag')\n",
        "  else:\n",
        "    aux = words.copy()\n",
        "    clearlists()\n",
        "    return aux\n",
        "\n",
        "# Retorna união de dois dataframes com \"frequência de documentos\" \n",
        "def datamerge(words0,words1,col1,col2):\n",
        "  merged = pd.merge(words0,words1, on=[col1,col2], how='outer')\n",
        "  merged = dataclean(merged).fillna(0)\n",
        "  merged['doc_freq'] = getdoc_freq(merged)\n",
        "  return merged.sort_values(by=[col1,col2])\n",
        "\n",
        "# Retorna lista de \"frequência de documentos\"\n",
        "def getdoc_freq(merged):\n",
        "  doc_freq = []\n",
        "  c = 0\n",
        "  # verifica duas colunas de frequência\n",
        "  for i in merged['freq_x'].tolist():\n",
        "    # se frequência maior que 0 em ambas colunas \n",
        "    if i > 0 and merged['freq_y'].tolist()[c] > 0:\n",
        "      doc_freq.append(2)\n",
        "    else:\n",
        "      doc_freq.append(1)\n",
        "    c += 1\n",
        "  return doc_freq\n",
        "\n",
        "# interseção dividida pela união com base na frequência de documentos\n",
        "def jaccard_simples(doc_freq):\n",
        "    intersecao = 0\n",
        "    c = 0\n",
        "    for x in doc_freq:\n",
        "      if x == 2:\n",
        "        intersecao += 1\n",
        "      c += 1\n",
        "    #print(\"interseção:\",intersecao)\n",
        "    uniao = len(doc_freq)\n",
        "    #print(\"união:\",uniao)\n",
        "    return intersecao/uniao\n",
        "\n",
        "# Não penderado, calculado a partir das frequências\n",
        "# De acordo com https://towardsdatascience.com/nlp-text-similarity-how-it-works-and-the-math-behind-it-a0fb90a05095\n",
        "def cosine_simples(mergedf):\n",
        "  numerador = 0\n",
        "  vetx = 0\n",
        "  vety = 0\n",
        "  for idx, row in mergedf.iterrows():\n",
        "    numerador += row['freq_x'] * row['freq_y']\n",
        "    vetx += pow(row['freq_x'],2)\n",
        "    vety += pow(row['freq_y'],2)\n",
        "  vetx = math.sqrt(vetx)\n",
        "  vety = math.sqrt(vety)\n",
        "  denominador = vetx * vety\n",
        "  return numerador/denominador\n",
        "\n",
        "# Similaridade com base na média das similaridades máximas entre palavras\n",
        "def similaridade_WP_max(str0,str1):\n",
        "  means = []\n",
        "  vals = []\n",
        "  aux = 0\n",
        "  for wd0 in str0:\n",
        "    syn0 = wn.synsets(wd0)\n",
        "    # verifica se 1a palavra contém alguma definição antes de prosseguir\n",
        "    if len(syn0) != 0:\n",
        "      vals.clear()\n",
        "      for wd1 in str1:\n",
        "        syn1 = wn.synsets(wd1)\n",
        "        # verifica se 2a palavra contém alguma definição antes de calcular similaridade\n",
        "        if len(syn1) != 0:\n",
        "          aux = syn0[0].wup_similarity(syn1[0])\n",
        "          # se similaridade = 1, palavras iguais, break\n",
        "          if aux == 1:\n",
        "            break\n",
        "          # senão, adiciona valor à lista vals\n",
        "          else:\n",
        "            vals.append(aux)\n",
        "            #print(\"syn0:\",syn0[0],\"syn1:\",syn1[0],\"wup:\",syn0[0].wup_similarity(syn1[0]))\n",
        "      # se aux = 1, similaridade máxima = 1\n",
        "      if aux == 1:\n",
        "         means.append(aux)\n",
        "      # senão seleciona máxima da lista\n",
        "      else:\n",
        "        means.append(max(vals))\n",
        "  #print(means)\n",
        "  return sum(means)/len(means)\n"
      ],
      "metadata": {
        "id": "e1C1u6qgrWtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retorna lemmas e hiperônimos das palavras contidas na lista do parâmetro\n",
        "def getclasses(stringlist):\n",
        "  lemmas = []\n",
        "  hypernyms = []\n",
        "  na = 'N\\A'\n",
        "  for wd in stringlist:\n",
        "    syn = wn.synsets(wd)\n",
        "    if len(syn) > 0:\n",
        "      lemmas.append(syn[0].lemmas()[0].name())\n",
        "      if len(syn[0].hypernyms()) > 0:\n",
        "        hypernyms.append(syn[0].hypernyms()[0].name())\n",
        "      else:\n",
        "        hypernyms.append(na)\n",
        "    else:\n",
        "      lemmas.append(na)\n",
        "      hypernyms.append(na)\n",
        "  return pd.DataFrame(zip(stringlist,lemmas,hypernyms),columns =['word','lemma','hypernym'])\n",
        "\n",
        "# Retorna tabela de frequências do tipo de classe (lemma ou hypernym), passado no parâmetro col\n",
        "def intersects(df0,df1,col):\n",
        "  aux = []\n",
        "  for i in df0.tolist():\n",
        "    if i != 'N\\A':\n",
        "      aux.append(i)\n",
        "  df0 = pd.DataFrame(zip(aux),columns =[col])\n",
        "  df0['freq'] = df0.groupby([col])[col].transform('count')\n",
        "  #print(df0)\n",
        "  aux.clear()\n",
        "  for i in df1.tolist():\n",
        "    if i != 'N\\A':\n",
        "      aux.append(i)\n",
        "  df1 = pd.DataFrame(zip(aux),columns =[col])\n",
        "  df1['freq'] = df1.groupby([col])[col].transform('count')\n",
        "  #print(df1)\n",
        "  return mergeclass(df0,df1,col)\n",
        "\n",
        "# Auxiliar à função intersects pra fazer o merge das tabelas do parâmetro\n",
        "def mergeclass(df0,df1,col):\n",
        "  merged = pd.merge(df0,df1, on=[col], how='outer')\n",
        "  merged = dataclean(merged).fillna(0)\n",
        "  merged['doc_freq'] = getdoc_freq(merged)\n",
        "  return merged.sort_values(by=[col])\n"
      ],
      "metadata": {
        "id": "nkon-CDeW5bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funções de comparação\n",
        "def compnomeadas(frase0,frase1,analise):\n",
        "  ner(preprocess(frase0))\n",
        "  dt0 = returndataset('entity','label')\n",
        "  ner(preprocess(frase1))\n",
        "  dt = datamerge(dt0,returndataset('entity','label'),'entity','label')\n",
        "  #print(dt)\n",
        "  if analise == 'j':\n",
        "    return jaccard_simples(dt['doc_freq'].tolist())\n",
        "  elif analise == 'c':\n",
        "    return cosine_simples(dt)\n",
        "  return None\n",
        "\n",
        "def comppostags(frase0,frase1,analise):\n",
        "  str0 = filtertuples(preprocess(frase0),True)\n",
        "  str1 = filtertuples(preprocess(frase1),True)\n",
        "  #print(\"string 0:\",str0['word'].tolist())\n",
        "  #print(\"string 1:\",str1['word'].tolist())\n",
        "  if analise == 'w':\n",
        "    return similaridade_WP_max(str0['word'].tolist(),str1['word'].tolist())\n",
        "  else:\n",
        "    dt = datamerge(str0,str1,'word','pos_tag')\n",
        "    if analise == 'j':\n",
        "      return jaccard_simples(dt['doc_freq'].tolist())\n",
        "    elif analise == 'c':\n",
        "      return cosine_simples(dt)\n",
        "  return None\n",
        "\n",
        "def compwordnet(frase0,frase1,classe,analise):\n",
        "  #print(preprocess(frase0))\n",
        "  #print(filtertuples(preprocess(frase0),False))\n",
        "  str0 = getclasses(filtertuples(preprocess(frase0),False))\n",
        "  #print(str0,'\\n')\n",
        "  str1 = getclasses(filtertuples(preprocess(frase1),False))\n",
        "  #print(str1,'\\n')\n",
        "  df = intersects(str0[classe],str1[classe],classe)\n",
        "  #print(df)\n",
        "  if analise == 'j':\n",
        "    return jaccard_simples(df['doc_freq'].tolist())\n",
        "  elif analise == 'c':\n",
        "    return cosine_simples(df)\n",
        "  return None\n",
        "\n",
        "def compscikit(frase0,frase1):\n",
        "  corpus = [frase0,frase1]\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  trsfm=vectorizer.fit_transform(corpus)\n",
        "  return cosine_similarity(trsfm[0:1], trsfm)[0][1]\n"
      ],
      "metadata": {
        "id": "3qeGsrP5hV1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testes com frases simples\n",
        "frases = [\"Airbnb will shut down its listings in China after two years of lockdowns in the country.\",\n",
        "        \"Starting this summer, Airbnb will take down its listings and offers for hosted experiences in China.\",\n",
        "        \"International brands, from Apple to Estee Lauder, have warned of the financial impact of the restrictions.\"]\n",
        "\n",
        "start = tm.time()\n",
        "print(\"\\n######## Comparação de Entidades Nomeadas ########\")\n",
        "print(\"\\n## strings 0 e 0 (caso base): ##\")\n",
        "print(compnomeadas(frases[0],frases[0],'c'))\n",
        "print(\"\\n## strings 0 e 1: ##\")\n",
        "print(compnomeadas(frases[0],frases[1],'c'))\n",
        "print(\"\\n## strings 0 e 2: ##\")\n",
        "print(compnomeadas(frases[0],frases[2],'c'))\n",
        "end = tm.time()\n",
        "print(\"\\n######## Tempo de execução:\", end-start,\"########\")\n",
        "\n",
        "start = tm.time()\n",
        "print(\"\\n######## Comparação de POS TAGS ########\")\n",
        "print(\"\\n## strings 0 e 0 (caso base): ##\")\n",
        "print(comppostags(frases[0],frases[0],'w'))\n",
        "print(\"\\n## strings 0 e 1: ##\")\n",
        "print(comppostags(frases[0],frases[1],'w'))\n",
        "print(\"\\n## strings 0 e 2: ##\")\n",
        "print(comppostags(frases[0],frases[2],'w'))\n",
        "end = tm.time()\n",
        "print(\"\\n######## Tempo de execução:\", end-start,\"########\")\n",
        "\n",
        "start = tm.time()\n",
        "print(\"\\n######## Comparação de POS TAGS ########\")\n",
        "print(\"\\n## strings 0 e 0 (caso base): ##\")\n",
        "print(comppostags(frases[0],frases[0],'c'))\n",
        "print(\"\\n## strings 0 e 1: ##\")\n",
        "print(comppostags(frases[0],frases[1],'c'))\n",
        "print(\"\\n## strings 0 e 2: ##\")\n",
        "print(comppostags(frases[0],frases[2],'c'))\n",
        "end = tm.time()\n",
        "print(\"\\n######## Tempo de execução:\", end-start,\"########\")\n",
        "\n",
        "# cosine do scikit\n",
        "start = tm.time()\n",
        "print('\\n######## Cosine do Scikit ########')\n",
        "print(\"\\n## strings 0 e 1: ##\")\n",
        "corpus = [frases[0],frases[1]]\n",
        "vectorizer = TfidfVectorizer()\n",
        "trsfm=vectorizer.fit_transform(corpus)\n",
        "print(cosine_similarity(trsfm[0:1], trsfm))\n",
        "print(\"\\n## strings 0 e 2: ##\")\n",
        "corpus = [frases[0],frases[2]]\n",
        "vectorizer = TfidfVectorizer()\n",
        "trsfm=vectorizer.fit_transform(corpus)\n",
        "print(cosine_similarity(trsfm[0:1], trsfm))\n",
        "print(\"\\n## strings 0, 1 e 2: ##\")\n",
        "corpus = [frases[0],frases[1],frases[2]]\n",
        "vectorizer = TfidfVectorizer()\n",
        "trsfm=vectorizer.fit_transform(corpus)\n",
        "print(cosine_similarity(trsfm[0:1], trsfm))\n",
        "end = tm.time()\n",
        "print(\"\\n######## Tempo de execução:\", end-start,\"########\")\n",
        "\n",
        "start = tm.time()\n",
        "print('\\n######## Comparação de Lemmas do Wordnet ########')\n",
        "print(\"\\n## strings 0 e 0 (caso base): ##\")\n",
        "print(compwordnet(frases[0],frases[0],'lemma','c'))\n",
        "print(\"\\n## strings 0 e 1: ##\")\n",
        "print(compwordnet(frases[0],frases[1],'lemma','c'))\n",
        "print(\"\\n## strings 0 e 2: ##\")\n",
        "print(compwordnet(frases[0],frases[2],'lemma','c'))\n",
        "end = tm.time()\n",
        "print(\"\\n######## Tempo de execução:\", end-start,\"########\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zw49iuWWb-m",
        "outputId": "24bc0603-4ee2-4a46-a03b-0d5b966f2694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "######## Comparação de Entidades Nomeadas ########\n",
            "\n",
            "## strings 0 e 0 (caso base): ##\n",
            "0.9999999999999998\n",
            "\n",
            "## strings 0 e 1: ##\n",
            "0.4999999999999999\n",
            "\n",
            "## strings 0 e 2: ##\n",
            "0.0\n",
            "\n",
            "######## Tempo de execução: 0.09979915618896484 ########\n",
            "\n",
            "######## Comparação de POS TAGS ########\n",
            "\n",
            "## strings 0 e 0 (caso base): ##\n",
            "1.0\n",
            "\n",
            "## strings 0 e 1: ##\n",
            "0.7073015873015873\n",
            "\n",
            "## strings 0 e 2: ##\n",
            "0.3337628384687208\n",
            "\n",
            "######## Tempo de execução: 0.06817936897277832 ########\n",
            "\n",
            "######## Comparação de POS TAGS ########\n",
            "\n",
            "## strings 0 e 0 (caso base): ##\n",
            "1.0000000000000002\n",
            "\n",
            "## strings 0 e 1: ##\n",
            "0.5000000000000001\n",
            "\n",
            "## strings 0 e 2: ##\n",
            "0.0\n",
            "\n",
            "######## Tempo de execução: 0.06440162658691406 ########\n",
            "\n",
            "######## Cosine do Scikit ########\n",
            "\n",
            "## strings 0 e 1: ##\n",
            "[[1.         0.31639145]]\n",
            "\n",
            "## strings 0 e 2: ##\n",
            "[[1.         0.12254598]]\n",
            "\n",
            "## strings 0, 1 e 2: ##\n",
            "[[1.         0.35608377 0.15772405]]\n",
            "\n",
            "######## Tempo de execução: 0.012107610702514648 ########\n",
            "\n",
            "######## Comparação de Lemmas do Wordnet ########\n",
            "\n",
            "## strings 0 e 0 (caso base): ##\n",
            "0.9999999999999998\n",
            "\n",
            "## strings 0 e 1: ##\n",
            "0.3999999999999999\n",
            "\n",
            "## strings 0 e 2: ##\n",
            "0.0\n",
            "\n",
            "######## Tempo de execução: 0.05306506156921387 ########\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1o. carregamento da base de dados de filmes\n",
        "urls = [\"https://raw.githubusercontent.com/GabrielFCosta/preprocessamento/main/movies.csv\",\n",
        "        \"https://raw.githubusercontent.com/GabrielFCosta/preprocessamento/main/ratings.csv\",\n",
        "        \"https://raw.githubusercontent.com/GabrielFCosta/preprocessamento/main/tags.csv\"]\n",
        "datasets = []\n",
        "auxlist =[]\n",
        "for url in urls:\n",
        "  df = pd.read_csv(url)\n",
        "  df[df.notnull()]\n",
        "  datasets.append(df)\n",
        "# concatena títulos e gêneros na coluna string\n",
        "datasets[0]['string'] = datasets[0]['title'] + \" \" + datasets[0]['genres']\n",
        "# substitui barras verticais '|' por espaços entre gêneros \n",
        "aux =''\n",
        "for idx1, i in datasets[0].iterrows():\n",
        "  aux = i['string'].replace('|', \" \")\n",
        "  auxlist.append(aux)\n",
        "datasets[0]['string'] = auxlist\n",
        "auxlist.clear()\n",
        "# concatena tags de usuários\n",
        "for idx1, i in datasets[0].iterrows():\n",
        "  df = datasets[2].loc[datasets[2]['movieId'] == i['movieId'] ] \n",
        "  aux =''\n",
        "  for idx2, j in df.iterrows():\n",
        "    aux = aux +\" \"+ j['tag']\n",
        "  #print(\"movie:\"+str(i['movieId'])+\" tags:\"+aux)\n",
        "  auxlist.append(aux)\n",
        "datasets[0]['string'] = datasets[0]['string'] + \"\" + auxlist\n"
      ],
      "metadata": {
        "id": "KPnYyw4MdM1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retorna dataframe de amostras de filmes aleatórios com rótulos de rating máximo\n",
        "def sampledata(df,sample,maxrated):\n",
        "  data = pd.DataFrame()\n",
        "  data = df.sample(n=sample)\n",
        "  data = data.astype({'movieId':'int'})\n",
        "  data[\"max_rated\"] = maxrated\n",
        "  return data\n",
        "\n",
        "# junta dois dataframes de 10 filmes de rating 5 com outros 40 aleatórios\n",
        "# retorna dataframe contendo movieIds e rótulos, ordenado por movieId\n",
        "def mergesamples(usersample,mids):\n",
        "  merged = pd.merge(usersample,mids, on=['movieId'], how='outer')\n",
        "  merged = dataclean(merged).fillna(0)\n",
        "  merged = merged.astype({'max_rated_x':'int'})\n",
        "  merged.rename(columns = {'max_rated_x':'max_rated'}, inplace = True)\n",
        "  merged.drop(['userId', 'rating', 'max_rated_y'], axis=1, inplace=True)\n",
        "  return merged.sort_values(by=['movieId'])\n",
        "\n",
        "# Retorna string do filme a partir do movieId\n",
        "def returnstring(movieId):\n",
        "  ref = datasets[0][datasets[0]['movieId'] == movieId]\n",
        "  return ref['string'].iloc[0]\n",
        "\n",
        "# 3o. Comparação seletiva\n",
        "def batchcompare(refmovie,testsample,comparison):\n",
        "  refmovie = returnstring(refmovie)\n",
        "  samplelist = []\n",
        "  valores = []\n",
        "  df  = pd.DataFrame()\n",
        "  df = testsample.copy(deep=True)\n",
        "  # coleta todas as strings dos filmes de teste na lista samplelist\n",
        "  for idx, row in testsample.iterrows():\n",
        "    samplelist.append(returnstring(row['movieId']))\n",
        "  # compara de acordo com o parâmetro comparison\n",
        "  if comparison == 'posw':\n",
        "    for frase in samplelist:\n",
        "      valores.append(comppostags(refmovie,frase,'w'))\n",
        "    df['postag_wp'] = valores\n",
        "    return df.sort_values(by=['postag_wp'], ascending = False).head(10)\n",
        "  elif comparison == 'posc':\n",
        "    for frase in samplelist:\n",
        "      valores.append(comppostags(refmovie,frase,'c'))\n",
        "    df['postag_cos'] = valores\n",
        "    return df.sort_values(by=['postag_cos'], ascending = False).head(10)\n",
        "  elif comparison == 'wnet':\n",
        "    for frase in samplelist:\n",
        "      valores.append(compwordnet(refmovie,frase,'lemma','c'))\n",
        "    df['wnet_lemma_cos'] = valores\n",
        "    return df.sort_values(by=['wnet_lemma_cos'], ascending = False).head(10)\n",
        "  elif comparison == 'scik':\n",
        "    for frase in samplelist:\n",
        "      valores.append(compscikit(refmovie,frase))\n",
        "    df['scikit_cos'] = valores\n",
        "    return df.sort_values(by=['scikit_cos'], ascending = False).head(10)\n",
        "  return None\n",
        "\n",
        "# lenta\n",
        "def getmovietitles(df):\n",
        "  titles =[]\n",
        "  for idx0, row0 in df.iterrows():\n",
        "    for idx1, row1 in datasets[0].iterrows():\n",
        "      if row0['movieId'] == row1['movieId']:\n",
        "        titles.append(row1['title'])\n",
        "        break\n",
        "  df['title'] = titles\n",
        "  return df\n",
        "\n",
        "# 2o. Análise em lotes\n",
        "def analyse(times):\n",
        "  for i in range(times):\n",
        "    print(\"\\n#### TESTE \"+str(i+1)+\" ####\")\n",
        "    # usuário deve ter mais de 10 filmes com avaliação 5\n",
        "    user =  pd.DataFrame()\n",
        "    while user.shape[0] < 11:\n",
        "      # sorteia usuário\n",
        "      id = random.randrange(1, 610)\n",
        "      # filtra todos os filmes de avaliação máxima do usuário sorteado\n",
        "      user = datasets[1].loc[(datasets[1]['userId'] == id) & (datasets[1]['rating'] == 5.0)]\n",
        "    print(\"userId: \"+str(id))\n",
        "    # sorteia 11 filmes de avaliação máxima do usuário\n",
        "    usersample = pd.DataFrame()\n",
        "    usersample = sampledata(user,11,1)\n",
        "    #print(usersample)\n",
        "    # pega todos os ids de filmes da base original\n",
        "    mids = pd.DataFrame()\n",
        "    mids['movieId'] = datasets[0]['movieId']\n",
        "    #print(\"total de filmes: \"+str(mids.shape[0]))\n",
        "    # pega todos os filmes do usuário\n",
        "    watched = pd.DataFrame()\n",
        "    watched = datasets[1].loc[(datasets[1]['userId'] == id)]\n",
        "    print(\"filmes assistidos: \"+str(watched.shape[0]))\n",
        "    # primeiro exclui todos filmes do usuário do total de filmes...\n",
        "    mask = mids['movieId'].isin(watched['movieId'].tolist())\n",
        "    mids = mids[~mask]\n",
        "    print(\"filmes filtrados: \"+str(mids.shape[0]))\n",
        "    # ...depois sorteia 10 outros filmes não assistidos dos que restaram\n",
        "    mids = sampledata(mids,10,0)\n",
        "    # dos 11 do usuário, sorteia 1 para servir de referência e o exclui do resto\n",
        "    reference =  pd.DataFrame()\n",
        "    reference = usersample.sample()\n",
        "    refmovie = reference['movieId'].iloc[0]\n",
        "    index = usersample[usersample['movieId'] == refmovie].index\n",
        "    usersample = usersample.drop(index)\n",
        "    print('movieId: '+str(refmovie))\n",
        "    print('string: '+returnstring(refmovie))\n",
        "    # dados de teste rotulados. 10 filmes de rating máximo mais 10 não assistidos\n",
        "    testsample = mergesamples(usersample,mids)\n",
        "    df = pd.DataFrame()\n",
        "    # tenta as análises sequencialmente\n",
        "    try:\n",
        "      df = batchcompare(refmovie,testsample,'posw') \n",
        "      print(\"PosTags com média de máximas WuPalmer: \" + str(calculataxa(df['max_rated'].tolist())))\n",
        "      print(df)\n",
        "    except:\n",
        "      print(\"PosTags com média de máximas WuPalmer não funcionou.\")\n",
        "    try:\n",
        "      df = batchcompare(refmovie,testsample,'posc')\n",
        "      print(\"PosTags com Cosine: \" + str(calculataxa(df['max_rated'].tolist())))\n",
        "      print(df)\n",
        "    except:\n",
        "      print(\"PosTags com Cosine não funcionou.\")\n",
        "    try:\n",
        "      df = batchcompare(refmovie,testsample,'wnet')\n",
        "      print(\"Lemmas do Wordnet com Cosine: \" + str(calculataxa(df['max_rated'].tolist())))\n",
        "      print(df)\n",
        "    except:\n",
        "      print(\"Lemmas do Wordnet com Cosine não funcionou.\")\n",
        "    try:\n",
        "      df = batchcompare(refmovie,testsample,'scik')\n",
        "      print(\"Cosine do Scikit: \" + str(calculataxa(df['max_rated'].tolist())))\n",
        "      print(df)\n",
        "    except:\n",
        "      print(\"Cosine do Scikit não funcionou.\")\n",
        "\n",
        "def calculataxa(lista):\n",
        "  c = 0\n",
        "  for i in lista:\n",
        "    if i == 1:\n",
        "      c+=1\n",
        "  return c/10\n",
        "\n",
        "analyse(3)\n",
        "\n",
        "#datasets[0].loc[datasets[0]['movieId'] == 1031]\n",
        "#datasets[1].loc[datasets[1]['userId'] == 143]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-_wgIhHMee5",
        "outputId": "7d78906b-9dae-4b23-f923-e73307634c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "#### TESTE 1 ####\n",
            "userId: 558\n",
            "filmes assistidos: 56\n",
            "filmes filtrados: 9686\n",
            "movieId: 3824\n",
            "string: Autumn in New York (2000) Drama Romance\n",
            "PosTags com média de máximas WuPalmer: 0.7\n",
            "    movieId  max_rated  postag_wp\n",
            "16     5012          0   0.594872\n",
            "3      4308          1   0.555983\n",
            "0      1222          1   0.520075\n",
            "7       597          1   0.497253\n",
            "8      3186          1   0.492727\n",
            "1      5064          1   0.453419\n",
            "9      1968          1   0.442892\n",
            "14    27820          0   0.442727\n",
            "10   136838          0   0.441697\n",
            "5      5420          1   0.422650\n",
            "PosTags com Cosine: 0.7\n",
            "    movieId  max_rated  postag_cos\n",
            "3      4308          1    0.400000\n",
            "16     5012          0    0.365148\n",
            "19    59118          0    0.316228\n",
            "10   136838          0    0.258199\n",
            "9      1968          1    0.223607\n",
            "5      5420          1    0.223607\n",
            "7       597          1    0.200000\n",
            "6      5329          1    0.200000\n",
            "0      1222          1    0.169031\n",
            "1      5064          1    0.141421\n",
            "Lemmas do Wordnet com Cosine: 0.7\n",
            "    movieId  max_rated  wnet_lemma_cos\n",
            "3      4308          1        0.447214\n",
            "16     5012          0        0.400000\n",
            "19    59118          0        0.316228\n",
            "10   136838          0        0.258199\n",
            "9      1968          1        0.223607\n",
            "6      5329          1        0.223607\n",
            "5      5420          1        0.223607\n",
            "7       597          1        0.200000\n",
            "0      1222          1        0.182574\n",
            "1      5064          1        0.149071\n",
            "Cosine do Scikit: 0.6\n",
            "    movieId  max_rated  scikit_cos\n",
            "3      4308          1    0.184432\n",
            "16     5012          0    0.155929\n",
            "5      5420          1    0.093495\n",
            "7       597          1    0.084580\n",
            "9      1968          1    0.084580\n",
            "10   136838          0    0.084580\n",
            "19    59118          0    0.084580\n",
            "6      5329          1    0.077809\n",
            "13   158388          0    0.068050\n",
            "0      1222          1    0.061231\n",
            "\n",
            "#### TESTE 2 ####\n",
            "userId: 287\n",
            "filmes assistidos: 152\n",
            "filmes filtrados: 9590\n",
            "movieId: 3949\n",
            "string: Requiem for a Dream (2000) Drama drug abuse depressing psychology\n",
            "PosTags com média de máximas WuPalmer: 0.4\n",
            "    movieId  max_rated  postag_wp\n",
            "17      953          0   0.539929\n",
            "14     7842          0   0.504575\n",
            "5       247          1   0.503175\n",
            "7      8949          1   0.500000\n",
            "12       40          0   0.486147\n",
            "8      4642          1   0.477031\n",
            "11     6041          0   0.471637\n",
            "0      6620          1   0.449242\n",
            "13     8521          0   0.441667\n",
            "15     2374          0   0.435256\n",
            "PosTags com Cosine: 0.6\n",
            "    movieId  max_rated  postag_cos\n",
            "4      5992          1    0.288675\n",
            "5       247          1    0.235702\n",
            "11     6041          0    0.204124\n",
            "7      8949          1    0.204124\n",
            "15     2374          0    0.204124\n",
            "14     7842          0    0.204124\n",
            "0      6620          1    0.182574\n",
            "1      4979          1    0.182574\n",
            "6      6711          1    0.182574\n",
            "17      953          0    0.166667\n",
            "Lemmas do Wordnet com Cosine: 0.6\n",
            "    movieId  max_rated  wnet_lemma_cos\n",
            "4      5992          1        0.288675\n",
            "11     6041          0        0.235702\n",
            "15     2374          0        0.235702\n",
            "14     7842          0        0.235702\n",
            "5       247          1        0.235702\n",
            "7      8949          1        0.204124\n",
            "13     8521          0        0.204124\n",
            "1      4979          1        0.204124\n",
            "0      6620          1        0.182574\n",
            "8      4642          1        0.182574\n",
            "Cosine do Scikit: 0.6\n",
            "    movieId  max_rated  scikit_cos\n",
            "14     7842          0    0.159764\n",
            "8      4642          1    0.126364\n",
            "15     2374          0    0.081768\n",
            "4      5992          1    0.081768\n",
            "11     6041          0    0.073971\n",
            "0      6620          1    0.073971\n",
            "7      8949          1    0.073971\n",
            "5       247          1    0.073971\n",
            "13     8521          0    0.063355\n",
            "1      4979          1    0.063355\n",
            "\n",
            "#### TESTE 3 ####\n",
            "userId: 450\n",
            "filmes assistidos: 51\n",
            "filmes filtrados: 9691\n",
            "movieId: 1673\n",
            "string: Boogie Nights (1997) Drama\n",
            "PosTags com média de máximas WuPalmer: 0.6\n",
            "    movieId  max_rated  postag_wp\n",
            "19     1300          0   0.653968\n",
            "10    49666          0   0.633333\n",
            "7      1193          1   0.633333\n",
            "1       260          1   0.624339\n",
            "2      1206          1   0.609524\n",
            "8      2028          1   0.609524\n",
            "9       110          1   0.577778\n",
            "17     1012          0   0.577778\n",
            "18     1678          0   0.577778\n",
            "3      3510          1   0.577778\n",
            "PosTags com Cosine: 0.3\n",
            "    movieId  max_rated  postag_cos\n",
            "12    43556          0    0.577350\n",
            "3      3510          1    0.333333\n",
            "17     1012          0    0.288675\n",
            "11     3150          0    0.288675\n",
            "14     3189          0    0.288675\n",
            "18     1678          0    0.258199\n",
            "7      1193          1    0.218218\n",
            "19     1300          0    0.218218\n",
            "2      1206          1    0.204124\n",
            "10    49666          0    0.204124\n",
            "Lemmas do Wordnet com Cosine: 0.3\n",
            "    movieId  max_rated  wnet_lemma_cos\n",
            "12    43556          0        0.577350\n",
            "3      3510          1        0.333333\n",
            "17     1012          0        0.288675\n",
            "11     3150          0        0.288675\n",
            "14     3189          0        0.288675\n",
            "18     1678          0        0.258199\n",
            "7      1193          1        0.235702\n",
            "19     1300          0        0.235702\n",
            "10    49666          0        0.235702\n",
            "2      1206          1        0.218218\n",
            "Cosine do Scikit: 0.3\n",
            "    movieId  max_rated  scikit_cos\n",
            "12    43556          0    0.170776\n",
            "3      3510          1    0.144384\n",
            "17     1012          0    0.127360\n",
            "11     3150          0    0.115216\n",
            "14     3189          0    0.115216\n",
            "18     1678          0    0.105992\n",
            "2      1206          1    0.092698\n",
            "19     1300          0    0.083409\n",
            "10    49666          0    0.083409\n",
            "8      2028          1    0.079703\n"
          ]
        }
      ]
    }
  ]
}